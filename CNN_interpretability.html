<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building and Visualizing CNNs: A Comprehensive Guide</title>
    <link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@300;400;700&family=Roboto:wght@300;400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
    <style>
        body {
            font-family: 'Merriweather', serif;
            color: #2e2e2c;
            line-height: 1.6;
            padding: 0;
            margin: 0;
        }
        .header {
            background: #e5e1d2;
            padding: 2rem 1rem;
            border-bottom: 1px solid #ccc;
        }
        .title {
            font-family: 'Roboto', sans-serif;
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
        }
        .subtitle {
            font-size: 1.25rem;
            color: #5a5a57;
        }
        .container {
            max-width: 800px;
            margin: 2rem auto;
            padding: 0 1rem;
            background: #fff;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            padding: 2rem;
            border-radius: 15px;
        }
        .blog-content h2 {
            font-family: 'Roboto', sans-serif;
            font-size: 2rem;
            color: #3a3a3a;
            margin-top: 2rem;
        }
        .blog-content p {
            margin: 1rem 0;
        }
        .blog-content blockquote {
            font-style: italic;
            color: #6c6c68;
            border-left: 4px solid #ccc;
            padding-left: 1rem;
            margin: 1.5rem 0;
            background: #f9f8f6;
            padding: 1rem;
            border-radius: 10px;
        }
        .ending-note {
            font-weight: bold;
            margin-top: 3rem;
        }
        .footer {
            background: #e5e1d2;
            text-align: center;
            padding: 1rem 0;
            margin-top: 2rem;
            border-top: 1px solid #ccc;
            border-radius: 15px 15px 0 0;
        }
        img {
            display: block;
            margin: 0 auto;
            max-width: 100%;
            height: auto;
        }
    </style>
</head>
<body>
    <header class="header">
        <div class="container">
            <h1 class="title">Let's Visualize How CNN Works</h1>
            <p class="subtitle">Trying to interpret things</p>
        </div>
    </header>

    <main class="container">
        <article class="blog-content">
            <h2>Introduction to CNNs</h2>
            <p>Convolutional Neural Networks (CNNs) have revolutionized the field of computer vision, enabling breakthroughs in image classification, object detection, and more. I recently started reading papers from scratch and during that I am trying to implement and understand things more deeply. This is partly inspired by the original AlexNet paper and Mechanistic Interpretability work by Chris Olah. I had many questions while reading the AlexNet paper, so I decided to write everything from scratch, and during that process, I got an idea to visualize things as I go.</p>

            <h2>My Takeaways from AlexNet</h2>
            <ul>
                <li><strong>Scale helps:</strong> The AlexNet paper demonstrated that increasing the size of the network (more parameters and deeper layers) significantly improved performance. The use of a much larger model compared to earlier networks showed that scale is crucial in achieving higher accuracy. The authors used 60 million parameters and 650,000 neurons, which was unprecedented at the time and allowed the network to learn complex patterns effectively.</li>
                <li><strong>GPUs are a big reason for today's models:</strong> The AlexNet architecture was one of the first to leverage GPUs for training, which drastically reduced training time and made it feasible to train deep networks on large datasets like ImageNet. This showed the importance of hardware acceleration in deep learning. Specifically, they used two NVIDIA GTX 580 GPUs to parallelize the training process, which enabled faster computation by splitting the network across both GPUs.</li>
                <li><strong>Data at scale, new dataset:</strong> AlexNet was trained on the ImageNet dataset, which consisted of over a million labeled images. The scale of the data was a key factor in its success, proving that having a large, diverse dataset is essential for training powerful models. The ImageNet dataset had 1.2 million training images distributed across 1,000 classes, allowing the model to learn a wide variety of features that generalized well.</li>
                <li><strong>The depth of the model at the time had a big impact:</strong> AlexNet had 8 layers, which was significantly deeper than previous models. This depth allowed the network to learn more complex features and hierarchical representations, contributing to its high accuracy. The architecture included 5 convolutional layers and 3 fully connected layers, which helped capture increasingly abstract features at each layer.</li>
                <li><strong>Tanh vs ReLU:</strong> The paper introduced the use of ReLU (Rectified Linear Units) instead of the traditional tanh or sigmoid activation functions. ReLU helped mitigate the vanishing gradient problem, enabling faster training and better convergence in deep networks. ReLU non-linearity allowed the model to train several times faster compared to using tanh, as it reduced the likelihood of gradients shrinking during backpropagation.</li>
                <li><strong>Normalization:</strong> The use of Local Response Normalization (LRN) in AlexNet helped improve generalization by normalizing the activities of neighboring neurons, which provided a form of lateral inhibition similar to what occurs in biological neurons. This was particularly beneficial for enhancing the generalization of the model in deeper layers and helped the network converge more reliably.</li>
                <li><strong>Overlapped pooling:</strong> Unlike earlier models, AlexNet used overlapping pooling layers, which helped reduce overfitting and improve the generalization of the model by adding more spatial context to each pooled feature. The authors used pooling windows of size 3x3 with a stride of 2, rather than non-overlapping pooling, which provided more robust feature maps by retaining more spatial information.</li>
                <li><strong>Dropout regularization:</strong> Dropout was introduced as a regularization technique to reduce overfitting by randomly dropping units during training. This helped the model generalize better by preventing neurons from co-adapting too much. Dropout was applied to the fully connected layers with a rate of 0.5, which significantly reduced overfitting and helped the model perform better on unseen data.</li>
            </ul>

            <h2>Original vs. Normalized Images</h2>
            <h4>Original Image</h4>
<p>- <strong>Description:</strong> The unaltered, raw input image with original pixel intensity values.<br>
- <strong>Features:</strong> Retains original colors, brightness, and contrast as captured or provided in the dataset.<br>
- <strong>Role:</strong> Serves as the initial input to the model without any preprocessing.</p>
<p><div style="text-align: center;"><div style="text-align: center;"><img src="images/CNN/original image for normalization.png" alt="Original Image"></div></div></p>

            <h4>Normalized Image</h4>
<p>During my experiments, I noticed how this normalization step makes a huge difference in training. By scaling pixel values to a standard range (typically between 0 and 1 or -1 and 1), we're essentially leveling the playing field for all our input data. It's like giving our CNN a standardized language to work with, rather than having it deal with wildly varying scales. This preprocessing step might seem simple, but it's one of those foundational concepts that every deep learning practitioner should understand. Having the mean centered at 0 is particularly valuable, as it helps maintain gradient symmetry and works especially well with activation functions like tanh that are centered around 0. This centering enables the network to learn both positive and negative correlations more easily while helping to mitigate floating point precision issues. Remember, the goal isn't just to make the images look different - it's about making them more suitable for our neural networks to process and learn from. Through visualization, we can better appreciate how this transformation affects our input data, even if the changes aren't always obvious to the human eye.
<br>
<p></p>
- <strong>Normalization Techniques:</strong>
  <ul>
    <li>Scaling pixel values between 0 and 1 or -1 and 1.</li>
    <li>Adjusting pixel intensities to standardize the data distribution.</li>
  </ul>
- <strong>Visual Differences:</strong> May appear darker or differently colored due to standardized pixel intensity ranges.<br>
- <strong>Purpose:</strong> Enhances model training stability and accelerates convergence by ensuring consistent input ranges.</p>
<p><img src="images/CNN/normalized image.png" alt="Normalized Image"></p>

            <p>I won't go much into building models because there are way better sources for that.</p>

            <h2>Model Architecture</h2>
<p>Creating a CNN involves stacking layers that progressively extract higher-level features from the input images. Below is a guide to building a simple CNN, with considerations for kernel sizes and padding.</p>
<pre><code>model = models.Sequential()

# Convolutional Layer 1
model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(height, width, channels)))
model.add(layers.MaxPooling2D((2, 2)))

# Convolutional Layer 2
model.add(layers.Conv2D(64, (5, 5), activation='relu', padding='same'))
model.add(layers.MaxPooling2D((2, 2)))

# Convolutional Layer 3
model.add(layers.Conv2D(128, (7, 7), activation='relu', padding='same'))
model.add(layers.MaxPooling2D((2, 2)))

# Fully Connected Layers
model.add(layers.Flatten())
model.add(layers.Dense(256, activation='relu'))
model.add(layers.Dense(num_classes, activation='softmax'))</code></pre>

            <h2>Visualizing with Grad-CAM</h2>
<h3>Grad-CAM Overview</h3>
<p><strong>Gradient-weighted Class Activation Mapping (Grad-CAM)</strong> is a technique that uses the gradients of any target concept (e.g., a specific class) flowing into the final convolutional layer to produce a localization map highlighting important regions in the image for predicting the concept.</p>

<p>During my deep dive into CNNs, one question kept nagging at me: how can we actually see what our model is looking at? This is where Gradient-weighted Class Activation Mapping (Grad-CAM) comes in, and it feels like getting X-ray vision into our neural networks.</p>

<h2>Kernel Analysis</h2>
<p>Different kernel sizes in CNNs affect the receptive field and feature extraction capabilities. Let's analyze how kernel size impacts the network's ability to capture patterns through activation maps and Grad-CAM visualizations:</p>

<ul>
    <li><strong>3x3 Kernels:</strong> These small kernels excel at capturing fine-grained, local features while being computationally efficient. They're ideal for detecting detailed textures and precise edges.
    <div style="text-align: center;">
        <img src="images/CNN/kernel3c1.png" alt="3x3 Kernel Visualization">
    </div>

    <li>This model with 3x3 kernels is taking arm of the cat as most important part to identify that it's a cat.
    <div style="text-align: center;">
        <img src="images/CNN/karnal3Grad.png" alt="Grad-CAM 3x3">
    </div>
    </li>
    <li><strong>5x5 Kernels:</strong> Medium-sized kernels provide a balance between local and global feature detection. They capture broader patterns while maintaining reasonable detail sensitivity.
    <div style="text-align: center;">
        <img src="images/CNN/kernal5conv1.png" alt="5x5 Kernel Visualization">
        <img src="images/CNN/kernal5grad.png" alt="Grad-CAM 5x5">
    </div>
    </li>
    <li><strong>7x7 Kernels:</strong> Larger kernels have a wider receptive field, making them better at capturing global patterns and larger-scale features at the cost of fine detail.
    <div style="text-align: center;">
        <img src="images/CNN/kernal7c1.png" alt="7x7 Kernel Visualization">
        <img src="images/CNN/gradkernal7.png" alt="Grad-CAM 7x7">
    </div>
</li>

    <li>This model with is giving importance to body shape or border of the cat.</li>
</ul>

<p>Key insights from kernel size comparison:</p>
<ul>
    <li><strong>Detail Sensitivity:</strong> Smaller kernels (3x3) preserve fine details but may miss broader context</li>
    <li><strong>Receptive Field:</strong> Larger kernels (7x7) capture wider spatial context but may lose local details</li>
    <li><strong>Computational Cost:</strong> Kernel size directly impacts the number of parameters and computational requirements</li>
    <li><strong>Feature Hierarchy:</strong> Different kernel sizes affect how the network builds its representation hierarchy</li>
</ul>
<h2>Padding Analysis</h2>
<p>By examining Grad-CAM visualizations with different padding configurations, we can understand how padding affects feature detection and model performance. Below are visualizations demonstrating the impact of padding settings:</p>
<ul>
    <li><strong>Padding 0 (No Padding):</strong> Features near the edges may be lost due to kernel operations truncating the input size. This can be problematic when important features are located near image boundaries.
    <div style="text-align: center;">
        <img src="images/CNN/convo1padding0.png" alt="No Padding Visualization">
        <img src="images/CNN/gradpadding0.png" alt="Grad-CAM No Padding">
    </div>
    </li>
    <li><strong>Padding 1:</strong> Preserves spatial dimensions and maintains feature detection capability at the edges. This is particularly useful for deep networks where maintaining spatial information is crucial.
    <div style="text-align: center;">
        <img src="images/CNN/convo1padding1.png" alt="Padding 1 Visualization">
        <img src="images/CNN/grad  padding1.png" alt="Grad-CAM Padding 1">
    </div>
    </li>
    <li>With padding it takes background to understand if image is related to dog or not. This type of situation and throw off your model results. But padding is usually good practice. </li>
</ul>

<p>Key observations from the padding comparison:</p>
<ul>
    <li><strong>Edge Information:</strong> With padding=1, the network preserves edge features that would otherwise be lost, leading to more complete feature extraction</li>
    <li><strong>Spatial Dimensions:</strong> Padding helps maintain the spatial dimensions of feature maps through multiple convolution layers</li>
    <li><strong>Feature Distribution:</strong> Notice how padding affects the distribution and intensity of activations, particularly near image boundaries</li>
</ul>
<h2>Stride Analysis</h2>
<p>Stride configuration significantly impacts feature detection and spatial resolution in CNNs. Let's examine how different stride values affect feature extraction through activation maps and Grad-CAM visualizations:</p>
<ul>
    <li><strong>Stride 1 (Default):</strong> Provides maximum spatial resolution by processing every possible position. This configuration captures fine-grained features but requires more computational resources.
    <div style="text-align: center;">
        <img src="images/CNN/stride1conv1.png" alt="Stride 1 Visualization">
        <img src="images/CNN/gradstride1.png" alt="Grad-CAM Stride 1">
    </div>
    </li>
    <li><strong>Stride 2:</strong> Reduces spatial dimensions by skipping every other pixel, leading to a more compact representation while potentially sacrificing some fine details.
    <div style="text-align: center;">
        <img src="images/CNN/stride2conv1.png" alt="Stride 2 Visualization">
        <img src="images/CNN/gradstride2.png" alt="Grad-CAM Stride 2">
    </div>
    </li>
</ul>

<p>Key observations from stride comparison:</p>
<ul>
    <li><strong>Resolution Trade-off:</strong> Higher stride values reduce spatial dimensions but may lose fine-grained details</li>
    <li><strong>Computational Efficiency:</strong> Increased stride reduces computational overhead by processing fewer spatial locations</li>
    <li><strong>Feature Hierarchy:</strong> Different stride configurations affect how the network builds its hierarchical representation of features</li>
</ul>


<h2>Conclusion</h2>
<p>Building and visualizing CNNs is an iterative process that requires careful consideration of various architectural components:</p>
<ul>
    <li><strong>Kernel Configurations:</strong> Whether using 3x3 for fine details or 7x7 for broader patterns, kernel size choices significantly impact feature extraction capabilities</li>
    <li><strong>Stride Settings:</strong> The balance between computational efficiency and spatial resolution determines how effectively your model processes information</li>
    <li><strong>Padding Options:</strong> Proper padding strategies help preserve spatial dimensions and maintain edge information throughout the network</li>
</ul>

<p>Through visualization techniques like Grad-CAM, we can:</p>
<ul>
    <li>Gain deeper insights into model decision-making processes</li>
    <li>Debug and optimize network architectures</li>
    <li>Make informed adjustments to improve model performance</li>
    <li>Better understand feature extraction hierarchies</li>
</ul>

<div class="next-steps">
    <h3>Next Steps</h3>
    <p>To continue your CNN journey:</p>
    <ul>
        <li>Experiment with different architectural configurations using the concepts discussed</li>
        <li>Apply these visualization techniques to your own models</li>
        <li>Explore how combining different configurations affects model performance</li>
        <li>Practice interpreting activation maps and Grad-CAM visualizations</li>
    </ul>
</div>

<div class="resources">
    <p><strong>For further reading, explore our resources and experiment with your own CNN models using the concepts discussed here.</strong></p>

    <div class="additional-resources">
        <h2>Additional Resources</h2>
        
        <h3>CNN Visualization Tools</h3>
        <ul>
            <li><strong>Grad-CAM Resources:</strong>
                <ul>
                    <li>Original Grad-CAM Paper: "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization"</li>
                    <li>PyTorch Implementation: <code>pytorch-grad-cam</code> library</li>
                    <li>TensorFlow Implementation: <code>tf-keras-vis</code> library</li>
                </ul>
            </li>
        </ul>
    
        <h3>Interactive Learning</h3>
        <ul>
            <li><strong>CNN Explainer:</strong> Interactive visualization of CNNs</li>
            <li><strong>TensorFlow Playground:</strong> Experiment with neural networks in browser</li>
            <li><strong>Distill.pub's CNN Articles:</strong> Visual, interactive explanations of CNNs</li>
        </ul>
    
        <h3>Implementation Guides</h3>
        <ul>
            <li><strong>Deep Learning Frameworks:</strong>
                <ul>
                    <li>PyTorch CNN Tutorials</li>
                    <li>TensorFlow CNN Guides</li>
                    <li>FastAI CNN Implementation Examples</li>
                </ul>
            </li>
        </ul>
    
        <h3>Further Reading</h3>
        <ul>
            <li><strong>Research Papers:</strong>
                <ul>
                    <li>"Visualizing and Understanding Convolutional Networks" by Zeiler and Fergus</li>
                    <li>"Deep Inside Convolutional Networks" by Simonyan et al.</li>
                    <li>"Understanding Deep Image Representations by Inverting Them"</li>
                </ul>
            </li>
        </ul>
    </div>
</div>


        </article>
    </main>



    <footer class="footer">
        <div class="container">
            <div class="footer-content" style="text-align: center;">
                <p>Ruchit Patel</p>
                <div class="social-links" style="display: flex; gap: 16px; justify-content: center;">
                    <a href="https://linkedin.com/in/rp440" target="_blank" rel="noopener noreferrer">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="#e5e1d2">
                            <path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/>
                        </svg>
                    </a>
                    <a href="https://x.com/ooltismile" target="_blank" rel="noopener noreferrer">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="#e5e1d2">
                            <path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/>
                        </svg>
                    </a>
                    <a href="https://github.com/rp440" target="_blank" rel="noopener noreferrer">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="#e5e1d2">
                            <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
                        </svg>
                    </a>
                </div>
            </div>
        </div>
    </footer>
</body>
</html>
